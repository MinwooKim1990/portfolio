{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\general2\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SST-2 dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 5.27k/5.27k [00:00<00:00, 9.55kB/s]\n",
      "Downloading data: 100%|██████████| 3.11M/3.11M [00:01<00:00, 1.74MB/s]\n",
      "Downloading data: 100%|██████████| 72.8k/72.8k [00:00<00:00, 119kB/s]\n",
      "Downloading data: 100%|██████████| 148k/148k [00:00<00:00, 236kB/s]\n",
      "Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 2820252.99 examples/s]\n",
      "Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 436552.05 examples/s]\n",
      "Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 825889.66 examples/s]\n",
      "c:\\Users\\user\\anaconda3\\envs\\general2\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\user\\anaconda3\\envs\\general2\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 67349/67349 [00:02<00:00, 27039.60 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 19572.28 examples/s]\n",
      "Map: 100%|██████████| 1821/1821 [00:00<00:00, 26460.61 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Preparation\n",
    "print(\"Loading SST-2 dataset...\")\n",
    "dataset = load_dataset(\"sst2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].with_format(\"torch\")\n",
    "test_dataset = tokenized_datasets[\"validation\"].with_format(\"torch\")  # Using validation set as test set\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Models\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # Transformer expects (seq_len, batch, features)\n",
    "        x = self.transformer(x)\n",
    "        x = x.mean(dim=0)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class SimpleSSM(nn.Module):\n",
    "    def __init__(self, d_model, d_state):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        self.A = nn.Parameter(torch.randn(d_state, d_state))\n",
    "        self.B = nn.Parameter(torch.randn(d_state, d_model))\n",
    "        self.C = nn.Parameter(torch.randn(d_model, d_state))\n",
    "        self.D = nn.Parameter(torch.randn(d_model))\n",
    "        \n",
    "    def forward(self, u):\n",
    "        # u: (batch, seq_len, d_model)\n",
    "        seq_len = u.size(1)\n",
    "        x = torch.zeros(u.size(0), self.d_state, device=u.device)\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            x = torch.tanh(self.A @ x.unsqueeze(-1) + self.B @ u[:, t, :].unsqueeze(-1)).squeeze(-1)\n",
    "            y = (self.C @ x.unsqueeze(-1)).squeeze(-1) + self.D * u[:, t, :]\n",
    "            outputs.append(y)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)\n",
    "\n",
    "class SimpleMamba(nn.Module):\n",
    "    def __init__(self, d_model, d_state, d_conv, expand):\n",
    "        super().__init__()\n",
    "        self.d_inner = expand * d_model\n",
    "        self.proj_in = nn.Linear(d_model, self.d_inner)\n",
    "        self.proj_out = nn.Linear(self.d_inner, d_model)\n",
    "        self.ssm = SimpleSSM(self.d_inner, d_state)\n",
    "        self.conv = nn.Conv1d(self.d_inner, self.d_inner, d_conv, padding=d_conv-1, groups=self.d_inner)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        x = self.proj_in(x)\n",
    "        x_ssm = self.ssm(x)\n",
    "        x_conv = self.conv(x.transpose(1, 2))[:, :, :x.size(1)].transpose(1, 2)\n",
    "        x = F.silu(x_ssm) * x_conv\n",
    "        return self.proj_out(x)\n",
    "\n",
    "class MambaModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, d_state, d_conv, expand, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.mamba = SimpleMamba(d_model, d_state, d_conv, expand)\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.mamba(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in dataloader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in dataloader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Training and Evaluation Functions\n",
    "def train(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            outputs = model(input_ids)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return total_loss / len(dataloader), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\general2\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Transformer model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\general2\\Lib\\site-packages\\torch\\nn\\functional.py:5504: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.5545\n",
      "Epoch 2/5, Loss: 0.3434\n",
      "Epoch 3/5, Loss: 0.2603\n",
      "Epoch 4/5, Loss: 0.2138\n",
      "Epoch 5/5, Loss: 0.1862\n",
      "Transformer Results:\n",
      "  Training Time: 41.68 seconds\n",
      "  Inference Time: 0.08 seconds\n",
      "  Test Accuracy: 0.7683\n",
      "\n",
      "Training Mamba model:\n",
      "Epoch 1/5, Loss: 0.5823\n",
      "Epoch 2/5, Loss: 0.3648\n",
      "Epoch 3/5, Loss: 0.2821\n",
      "Epoch 4/5, Loss: 0.2380\n",
      "Epoch 5/5, Loss: 0.2078\n",
      "Mamba Results:\n",
      "  Training Time: 556.68 seconds\n",
      "  Inference Time: 0.35 seconds\n",
      "  Test Accuracy: 0.7901\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Main Comparison\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "NUM_CLASSES = 2  # Binary classification\n",
    "NUM_EPOCHS = 5\n",
    "D_STATE = 16\n",
    "D_CONV = 4\n",
    "EXPAND = 2\n",
    "\n",
    "# Initialize models\n",
    "transformer_model = TransformerModel(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, NUM_CLASSES).to(device)\n",
    "mamba_model = MambaModel(VOCAB_SIZE, EMBED_DIM, D_STATE, D_CONV, EXPAND, NUM_CLASSES).to(device)\n",
    "\n",
    "# Training loop\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "transformer_optimizer = optim.Adam(transformer_model.parameters())\n",
    "mamba_optimizer = optim.Adam(mamba_model.parameters())\n",
    "\n",
    "results = {\n",
    "    \"transformer\": {\"train_time\": 0, \"inference_time\": 0, \"accuracy\": 0},\n",
    "    \"mamba\": {\"train_time\": 0, \"inference_time\": 0, \"accuracy\": 0}\n",
    "}\n",
    "\n",
    "for model_name, model, optimizer in [(\"transformer\", transformer_model, transformer_optimizer),\n",
    "                                     (\"mamba\", mamba_model, mamba_optimizer)]:\n",
    "    print(f\"\\nTraining {model_name.capitalize()} model:\")\n",
    "    train_start = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss = train(model, train_dataloader, criterion, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss:.4f}\")\n",
    "    train_time = time.time() - train_start\n",
    "    results[model_name][\"train_time\"] = train_time\n",
    "    \n",
    "    inference_start = time.time()\n",
    "    test_loss, accuracy = evaluate(model, test_dataloader, criterion, device)\n",
    "    inference_time = time.time() - inference_start\n",
    "    results[model_name][\"inference_time\"] = inference_time\n",
    "    results[model_name][\"accuracy\"] = accuracy\n",
    "    \n",
    "    print(f\"{model_name.capitalize()} Results:\")\n",
    "    print(f\"  Training Time: {train_time:.2f} seconds\")\n",
    "    print(f\"  Inference Time: {inference_time:.2f} seconds\")\n",
    "    print(f\"  Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Comparison:\n",
      "Transformer vs Mamba:\n",
      "  Training Time: 41.68s vs 556.68s\n",
      "  Inference Time: 0.08s vs 0.35s\n",
      "  Accuracy: 0.7683 vs 0.7901\n"
     ]
    }
   ],
   "source": [
    "# Final Comparison\n",
    "print(\"\\nFinal Comparison:\")\n",
    "print(f\"Transformer vs Mamba:\")\n",
    "print(f\"  Training Time: {results['transformer']['train_time']:.2f}s vs {results['mamba']['train_time']:.2f}s\")\n",
    "print(f\"  Inference Time: {results['transformer']['inference_time']:.2f}s vs {results['mamba']['inference_time']:.2f}s\")\n",
    "print(f\"  Accuracy: {results['transformer']['accuracy']:.4f} vs {results['mamba']['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
